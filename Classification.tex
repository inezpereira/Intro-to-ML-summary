\section*{Classification}
\subsection*{0/1 loss}
0/1 loss is not convex and not differentiable.\\
$l_{0/1} (w;y_i,x_i) =
\begin{cases}
    1 \text{ , if } y_i \neq sign(w^Tx_i)\\
		0 \text{ , otherwise} 
\end{cases}$

\subsection*{Perceptron loss}
Perceptron loss is convex and gradient is informative.\\
$l_{P} (w;y_i,x_i) = max\{0, -y_i w^T x_i \}$\\
$\nabla_w l_p(w;y_i,x_i) = \begin{cases}
    0 &\text{ , if } y_i w^T x_i \geq 0\\
    -y_i x_i &\text{ , if } y_i w^T x_i <0
\end{cases}\\
w^* = \underset{w}{\operatorname{argmin}} \sum_{i=1}^n l_p (w;y_i,x_i)$

\subsection*{Stochastic Gradient Descent (SGD)}
1. Start at an arbitrary $w_0 \in \mathbb{R}^d$ and pick $\eta_t$\\
2. For $t = 1, 2,  ...$ do: \\
	Pick one data point $(x',y') \in_{u.a.r.} D$ (replace=T)\\
	$w_{t+1} = w_t - \eta_t \nabla_w l(w_t;x',y')$\\
Vs. Perceptron Algorithm: SGD with Perceptron loss (need if else statement here).
For mini-batch SGD (size n): $\nabla_w G(w) = \frac{1}{n}\sum_{i=1}^n\nabla_w g_i(w)$ 

%\subsection*{Perceptron Algorithm}
%Stoch. Gradient Descent with Perceptron loss\\
%\emph{Theorem:} If $D$ is linearly separable $\Rightarrow$ Perceptron will obtain a linear separator.

%\subsection*{Hinge loss}
%Loss for Support Vector Machine.\\
%$l_H(w;x,y) = max \{0,1-y w^T x\}$

\subsection*{Support Vector Machine}
Hinge loss: $l_H(w;x,y) = max \{0,1-y w^T x\}$\\
Goal: get 0 loss and maximize ``margins''.\\
$w^* = \underset{w}{\operatorname{argmin}} \sum_{i=1}^n  max \{0,1-y_i w^T x_i\} + \lambda ||w||_2^2\\
g_i(w) = max \{0,1-y_i w^T x_i\} + \lambda ||w||_2^2\\
\nabla_w g_i(w) = \begin{cases}
    -y_i x_i + 2\lambda w &\text{ , if $y_i w^T x_i<1$}\\
		2\lambda w &\text{ , if $y_i w^T x_i \geq 1$}
\end{cases}$

\subsection*{L1-SVM}
$\underset{w}{\operatorname{min}} \sum_{i=1}^n max(0,1-y_i w^T x_i)$+$\lambda ||w||_1$ 

%\subsection*{Matrix-Vector Gradient}
%multiply transposed matrix to the same side as its occurance w.r.t. derivate variable: $\beta \in \mathbb{R}^d$
%$\nabla_\beta ( ||y-X\beta||_2^2 + \lambda ||\beta||_2^2 ) = 2X^T (y-X\beta) + 2\lambda \beta$\\
